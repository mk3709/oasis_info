{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-21T17:13:20.863203Z","iopub.execute_input":"2023-06-21T17:13:20.863563Z","iopub.status.idle":"2023-06-21T17:13:20.885246Z","shell.execute_reply.started":"2023-06-21T17:13:20.863534Z","shell.execute_reply":"2023-06-21T17:13:20.883425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:20.890055Z","iopub.execute_input":"2023-06-21T17:13:20.890877Z","iopub.status.idle":"2023-06-21T17:13:22.220047Z","shell.execute_reply.started":"2023-06-21T17:13:20.890849Z","shell.execute_reply":"2023-06-21T17:13:22.21906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\",sep=\",\",encoding=\"ISO-8859-1\")\ndf","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:22.222033Z","iopub.execute_input":"2023-06-21T17:13:22.222425Z","iopub.status.idle":"2023-06-21T17:13:22.289422Z","shell.execute_reply.started":"2023-06-21T17:13:22.222375Z","shell.execute_reply":"2023-06-21T17:13:22.288321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:22.290789Z","iopub.execute_input":"2023-06-21T17:13:22.291161Z","iopub.status.idle":"2023-06-21T17:13:22.299474Z","shell.execute_reply.started":"2023-06-21T17:13:22.291126Z","shell.execute_reply":"2023-06-21T17:13:22.298436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:22.302942Z","iopub.execute_input":"2023-06-21T17:13:22.303887Z","iopub.status.idle":"2023-06-21T17:13:22.312097Z","shell.execute_reply.started":"2023-06-21T17:13:22.303852Z","shell.execute_reply":"2023-06-21T17:13:22.311324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename({\"v1\":\"label\",\"v2\":\"message\"},axis=1,inplace=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:22.313792Z","iopub.execute_input":"2023-06-21T17:13:22.314616Z","iopub.status.idle":"2023-06-21T17:13:22.333987Z","shell.execute_reply.started":"2023-06-21T17:13:22.314577Z","shell.execute_reply":"2023-06-21T17:13:22.332915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To check the the dataset is Balanced or Imbalaced \ndf[\"label\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:22.336314Z","iopub.execute_input":"2023-06-21T17:13:22.337014Z","iopub.status.idle":"2023-06-21T17:13:22.34987Z","shell.execute_reply.started":"2023-06-21T17:13:22.336982Z","shell.execute_reply":"2023-06-21T17:13:22.348708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=df[\"label\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:22.352079Z","iopub.execute_input":"2023-06-21T17:13:22.352527Z","iopub.status.idle":"2023-06-21T17:13:22.603696Z","shell.execute_reply.started":"2023-06-21T17:13:22.352489Z","shell.execute_reply":"2023-06-21T17:13:22.601428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from the above plot we can see that dataset is imbalanced","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import *\n\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:22.605337Z","iopub.execute_input":"2023-06-21T17:13:22.60571Z","iopub.status.idle":"2023-06-21T17:13:23.338524Z","shell.execute_reply.started":"2023-06-21T17:13:22.605677Z","shell.execute_reply":"2023-06-21T17:13:23.337568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=[]\nstemmer=PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:23.339841Z","iopub.execute_input":"2023-06-21T17:13:23.340273Z","iopub.status.idle":"2023-06-21T17:13:23.345384Z","shell.execute_reply.started":"2023-06-21T17:13:23.34024Z","shell.execute_reply":"2023-06-21T17:13:23.344438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=[]\nfor i in range(len(df['message'])):\n    review=re.sub('[^a-zA-Z]',' ',df['message'][i]) ## First remove the non word characters\n    review=review.lower()\n    review=review.split()\n    review=[stemmer.stem(word) for word in review if not word in stopwords.words(\"english\")]\n    review=\" \".join(review)\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:23.350667Z","iopub.execute_input":"2023-06-21T17:13:23.351202Z","iopub.status.idle":"2023-06-21T17:13:38.287457Z","shell.execute_reply.started":"2023-06-21T17:13:23.351178Z","shell.execute_reply":"2023-06-21T17:13:38.286451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus[5]","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:38.28881Z","iopub.execute_input":"2023-06-21T17:13:38.289815Z","iopub.status.idle":"2023-06-21T17:13:38.296219Z","shell.execute_reply.started":"2023-06-21T17:13:38.28978Z","shell.execute_reply":"2023-06-21T17:13:38.295221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bag of Words Model for Text representation of independenet feature","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features=5000) \n## we are just trying top 5000 Words,then we can tune it accoring to accuracy and other matrix\nX = vectorizer.fit_transform(corpus).toarray()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:38.297867Z","iopub.execute_input":"2023-06-21T17:13:38.298486Z","iopub.status.idle":"2023-06-21T17:13:38.477982Z","shell.execute_reply.started":"2023-06-21T17:13:38.29845Z","shell.execute_reply":"2023-06-21T17:13:38.47697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## This is Sparsed Matrix of Dependent features \nX,X.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:38.479723Z","iopub.execute_input":"2023-06-21T17:13:38.480117Z","iopub.status.idle":"2023-06-21T17:13:38.489271Z","shell.execute_reply.started":"2023-06-21T17:13:38.480085Z","shell.execute_reply":"2023-06-21T17:13:38.488075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"label\"].unique()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:38.490885Z","iopub.execute_input":"2023-06-21T17:13:38.491672Z","iopub.status.idle":"2023-06-21T17:13:38.50223Z","shell.execute_reply.started":"2023-06-21T17:13:38.491634Z","shell.execute_reply":"2023-06-21T17:13:38.501094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We treansformed the dependent features but our Indpendent(Target) feature is yet to be converted into numerical.\n - Because this column has only 2 Categories we can try one-Hot Encoding or pd.get_dummies\n","metadata":{}},{"cell_type":"code","source":"y=pd.get_dummies(df[\"label\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:38.5039Z","iopub.execute_input":"2023-06-21T17:13:38.504426Z","iopub.status.idle":"2023-06-21T17:13:38.513663Z","shell.execute_reply.started":"2023-06-21T17:13:38.504374Z","shell.execute_reply":"2023-06-21T17:13:38.512623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:38.515539Z","iopub.execute_input":"2023-06-21T17:13:38.515877Z","iopub.status.idle":"2023-06-21T17:13:38.529425Z","shell.execute_reply.started":"2023-06-21T17:13:38.515846Z","shell.execute_reply":"2023-06-21T17:13:38.527945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can drop one column because we can predict both results in one column only and we can do that in pd.get_dummies function","metadata":{}},{"cell_type":"code","source":"## Now we are getting the Spam Column into the model\ny=pd.get_dummies(df[\"label\"],drop_first=True)\ny","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:38.531249Z","iopub.execute_input":"2023-06-21T17:13:38.531635Z","iopub.status.idle":"2023-06-21T17:13:38.546535Z","shell.execute_reply.started":"2023-06-21T17:13:38.531601Z","shell.execute_reply":"2023-06-21T17:13:38.545437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian Naive Bayes\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:38.548454Z","iopub.execute_input":"2023-06-21T17:13:38.548825Z","iopub.status.idle":"2023-06-21T17:13:39.151277Z","shell.execute_reply.started":"2023-06-21T17:13:38.548791Z","shell.execute_reply":"2023-06-21T17:13:39.150249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To get unique values in test data and total size of test data\ny_test.value_counts(),len(y_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.152923Z","iopub.execute_input":"2023-06-21T17:13:39.153447Z","iopub.status.idle":"2023-06-21T17:13:39.167473Z","shell.execute_reply.started":"2023-06-21T17:13:39.153396Z","shell.execute_reply":"2023-06-21T17:13:39.166425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### In the Test data we have  0 Means Ham and 1 Means Spam and we have 219 Instance of Spam Entries and 1453 instance of Ham Entries ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import *\n\nacc=accuracy_score(y_test, y_pred)\ncm=confusion_matrix(y_test, y_pred)\na=precision_recall_fscore_support(y_test, y_pred, average='binary')\n\nprint(f'Gaussian Naive Bayes Model is Giving These metrics after Text \"Representation by BOW Model\", \\n\\n Accuracy of this model is {acc} ,\\n\\n Confusion Matrix for this model \\n\\n{cm}, \\n\\n Precision,Recall,F-1 score for Binary Average of data are \\n\\n{a} \\n\\n')\ntarget_names = ['ham', 'spam']\nreport = classification_report(y_test, y_pred, target_names=target_names,labels=None)\n\nprint(f\"Classification report for this model \\n\\n{report}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.169366Z","iopub.execute_input":"2023-06-21T17:13:39.169866Z","iopub.status.idle":"2023-06-21T17:13:39.211547Z","shell.execute_reply.started":"2023-06-21T17:13:39.16983Z","shell.execute_reply":"2023-06-21T17:13:39.210275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Precision: It measures the proportion of correctly predicted instances out of the total predicted instances for each class.\n\n2. Recall: It measures the proportion of correctly predicted instances out of the total actual instances for each class.\n\n3. F1-score: It is the harmonic mean of precision and recall and provides a balanced measure of a classifier's performance.\n\n4. Support: It represents the number of instances in each class.\n\n5. Accuracy: It indicates the overall accuracy of the classifier.\n### 'binary' average:\nIf your focus is solely on evaluating the performance of identifying spam instances, you can use the 'binary' average. This average calculates the F1 score specifically for the positive class (spam) while treating the negative class as non-spam. It is suitable if your primary concern is the accuracy of spam detection.\n\n### 'macro' or 'weighted' average: \nIf you want to evaluate the model's performance across both spam and non-spam classes, you can use the 'macro' or 'weighted' average. The 'macro' average treats both classes equally, while the 'weighted' average takes into account class imbalance and varying sample sizes. These averages provide an overall assessment of the model's performance, considering both precision and recall for both classes.\n\n### Reason for Changes in Precision and recall for each average \n\nWhen you change the average parameter in the F1 score calculation, it can affect the precision and recall values, which in turn impacts the resulting F1 score. Here's why the precision and recall may change when you switch between different averaging options:\n\n### 1. 'binary' average:\n\nPrecision and recall are calculated specifically for the positive class (as specified by pos_label), treating the negative class as non-positive\n.\nPrecision: The proportion of true positive predictions out of all positive predictions. Only predictions related to the positive class are considered.\n\nRecall: The proportion of true positive predictions out of all actual positive instances. Only instances related to the positive class are considered.\n\n### 2. 'macro' average:\n\nPrecision and recall are calculated independently for each class and then averaged.\n\nPrecision: The average of precision values calculated for each class.\n\nRecall: The average of recall values calculated for each class.\n### 3. 'weighted' average:\n\nPrecision and recall are calculated independently for each class and then weighted by the number of samples in each class before averaging.\n\nPrecision: The weighted average of precision values, considering the class imbalance.\n\nRecall: The weighted average of recall values, considering the class imbalance.\nWhen you switch between these averaging options, the way precision and recall are computed changes, leading to potential variations in their values. \n\nThe choice of averaging option can affect how much weight is given to each class and how the contributions of different classes are combined to calculate the final precision and recall values.\n\nIt's worth noting that depending on the class distribution and the performance of your model, different averaging options may result in different F1 scores. It's important to interpret and consider the implications of each average in the context of your specific problem and the evaluation requirements you have.","metadata":{}},{"cell_type":"markdown","source":"#### We are getting the 86% , if we provide \"spam\" column as the \"Target(y)\" Column, we can change the column for \"ham\" and will check for accuracy, because , the ham frequency is greater than spam","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.213227Z","iopub.execute_input":"2023-06-21T17:13:39.213965Z","iopub.status.idle":"2023-06-21T17:13:39.229148Z","shell.execute_reply.started":"2023-06-21T17:13:39.213924Z","shell.execute_reply":"2023-06-21T17:13:39.227867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Now we are getting the Spam Column into the model\ny=pd.get_dummies(df[\"label\"],drop_first=True)\ny","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.231151Z","iopub.execute_input":"2023-06-21T17:13:39.231573Z","iopub.status.idle":"2023-06-21T17:13:39.245593Z","shell.execute_reply.started":"2023-06-21T17:13:39.231536Z","shell.execute_reply":"2023-06-21T17:13:39.244423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To check the accuracy of model changes if we provide 1 for spam\ny.replace([0,1],[1,0],inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.247569Z","iopub.execute_input":"2023-06-21T17:13:39.247963Z","iopub.status.idle":"2023-06-21T17:13:39.254658Z","shell.execute_reply.started":"2023-06-21T17:13:39.247926Z","shell.execute_reply":"2023-06-21T17:13:39.253304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.256693Z","iopub.execute_input":"2023-06-21T17:13:39.257501Z","iopub.status.idle":"2023-06-21T17:13:39.802763Z","shell.execute_reply.started":"2023-06-21T17:13:39.257463Z","shell.execute_reply":"2023-06-21T17:13:39.801588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.804266Z","iopub.execute_input":"2023-06-21T17:13:39.805166Z","iopub.status.idle":"2023-06-21T17:13:39.818575Z","shell.execute_reply.started":"2023-06-21T17:13:39.805131Z","shell.execute_reply":"2023-06-21T17:13:39.817122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### In the Test data we have  0 Means Spam and 1 Means Ham and we have 219 Instance of Spam Entries and 1453 instance of Ham Entries ","metadata":{}},{"cell_type":"code","source":"acc=accuracy_score(y_test, y_pred)\ncm=confusion_matrix(y_test, y_pred)\na=precision_recall_fscore_support(y_test, y_pred, average='binary')\n\nprint(f'Gaussian Naive Bayes Model is Giving These metrics after Text \"Representation by BOW Model\", \\n\\n Accuracy of this model is {acc} ,\\n\\n Confusion Matrix for this model \\n\\n{cm}, \\n\\n Precision,Recall,F-1 score for Binary Average of data are \\n\\n{a} \\n\\n')\ntarget_names = ['Spam', 'Ham']\nreport = classification_report(y_test, y_pred, target_names=target_names,labels=None)\n\nprint(f\"Classification report for this model \\n\\n{report}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.820188Z","iopub.execute_input":"2023-06-21T17:13:39.820996Z","iopub.status.idle":"2023-06-21T17:13:39.892026Z","shell.execute_reply.started":"2023-06-21T17:13:39.820942Z","shell.execute_reply":"2023-06-21T17:13:39.891028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We are able to increase the accuracy from 86% to 88% ,\n## Let's see what else we can tune the parameters so that we can increase the accuracy of the model\n- stemming into lemmatisation  \n- max_features(Top Features) in the model\n- changing BOW model into TF-IDF Model","metadata":{}},{"cell_type":"code","source":"## Now we are getting the Spam Column into the model\ny=pd.get_dummies(df[\"label\"],drop_first=True)\ny","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.89348Z","iopub.execute_input":"2023-06-21T17:13:39.896877Z","iopub.status.idle":"2023-06-21T17:13:39.914584Z","shell.execute_reply.started":"2023-06-21T17:13:39.896833Z","shell.execute_reply":"2023-06-21T17:13:39.912556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To check the accuracy of model changes if we provide 1 for spam\ny.replace([0,1],[1,0],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.932907Z","iopub.execute_input":"2023-06-21T17:13:39.934515Z","iopub.status.idle":"2023-06-21T17:13:39.956662Z","shell.execute_reply.started":"2023-06-21T17:13:39.934426Z","shell.execute_reply":"2023-06-21T17:13:39.946796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We are checking for TF-IDF model","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import *\nvectorizer =TfidfVectorizer(max_features=3000) \n## we are just trying top 5000 Words,then we can tune it accoring to accuracy and other matrix\nX = vectorizer.fit_transform(corpus).toarray()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:39.957806Z","iopub.execute_input":"2023-06-21T17:13:39.958164Z","iopub.status.idle":"2023-06-21T17:13:40.277562Z","shell.execute_reply.started":"2023-06-21T17:13:39.958131Z","shell.execute_reply":"2023-06-21T17:13:40.276474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Sparse Matrix after TF-IDF Converting\nX","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:40.282691Z","iopub.execute_input":"2023-06-21T17:13:40.28585Z","iopub.status.idle":"2023-06-21T17:13:40.296881Z","shell.execute_reply.started":"2023-06-21T17:13:40.285812Z","shell.execute_reply":"2023-06-21T17:13:40.295778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\n\nacc=accuracy_score(y_test, y_pred)\ncm=confusion_matrix(y_test, y_pred)\na=precision_recall_fscore_support(y_test, y_pred, average='binary')\n\nprint(f'Gaussian Naive Bayes Model is Giving These metrics after \"Text Representation by TF-IDF Model\", \\n\\n Accuracy of this model is {acc} ,\\n\\n Confusion Matrix for this model \\n\\n{cm}, \\n\\n Precision,Recall,F-1 score for Binary Average of data are \\n\\n{a} \\n\\n')\ntarget_names = ['Spam', 'Ham']\nreport = classification_report(y_test, y_pred, target_names=target_names,labels=None)\n\nprint(f\"Classification report for this model \\n\\n{report}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:40.301657Z","iopub.execute_input":"2023-06-21T17:13:40.304223Z","iopub.status.idle":"2023-06-21T17:13:40.744607Z","shell.execute_reply.started":"2023-06-21T17:13:40.304177Z","shell.execute_reply":"2023-06-21T17:13:40.74353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### By changing the  Text Representation(Encoding) of features from BOW  Model to TF-IDF Model ,we are getting the same results as BOW Model Text Representaion\n\n- Lets change the Classification model and check","metadata":{}},{"cell_type":"markdown","source":"## Multinomial Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import *\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\nmnb = MultinomialNB ()\ny_pred = mnb.fit(X_train, y_train).predict(X_test)\n\n## Metrics to Check the Model Performance\nacc=accuracy_score(y_test, y_pred)\ncm=confusion_matrix(y_test, y_pred)\na=precision_recall_fscore_support(y_test, y_pred, average='binary')\n\nprint(f'Gaussian Naive Bayes Model is Giving These metrics after \"Text Representation by TF-IDF Model\", \\n\\n Accuracy of this model is {acc} ,\\n\\n Confusion Matrix for this model \\n\\n{cm}, \\n\\n Precision,Recall,F-1 score for Binary Average of data are \\n\\n{a} \\n\\n')\ntarget_names = ['Spam', 'Ham']\nreport = classification_report(y_test, y_pred, target_names=target_names,labels=None)\n\nprint(f\"Classification report for this model \\n\\n {report}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:40.749291Z","iopub.execute_input":"2023-06-21T17:13:40.751758Z","iopub.status.idle":"2023-06-21T17:13:40.973412Z","shell.execute_reply.started":"2023-06-21T17:13:40.751703Z","shell.execute_reply":"2023-06-21T17:13:40.972414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We are getting better results in Multinomial Naive Bayes Model, lets check for other algorithms also","metadata":{}},{"cell_type":"markdown","source":"### Complement Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import *\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\ncnb = ComplementNB()\ny_pred = cnb.fit(X_train, y_train).predict(X_test)\naccuracy_score(y_test, y_pred),confusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:40.978686Z","iopub.execute_input":"2023-06-21T17:13:40.982182Z","iopub.status.idle":"2023-06-21T17:13:41.152896Z","shell.execute_reply.started":"2023-06-21T17:13:40.982143Z","shell.execute_reply":"2023-06-21T17:13:41.151881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We are getting accuracy of 92% which is below multinominal naive bayes model","metadata":{}},{"cell_type":"markdown","source":"###  Bernoulli Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import *\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\nbnb = BernoulliNB()\ny_pred = bnb.fit(X_train, y_train).predict(X_test)\naccuracy_score(y_test, y_pred),confusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:41.158271Z","iopub.execute_input":"2023-06-21T17:13:41.161585Z","iopub.status.idle":"2023-06-21T17:13:41.505503Z","shell.execute_reply.started":"2023-06-21T17:13:41.161547Z","shell.execute_reply":"2023-06-21T17:13:41.504075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### we are getting better results from  Bernoulli Naive Bayes Model which is giving 97.78%, so we are finalising this model \n\n### But  lets see we can increase accuracy and decrease False positives and False Negatives little bit by changing text representation part from stemming into lemmatization","metadata":{}},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:41.511713Z","iopub.execute_input":"2023-06-21T17:13:41.512747Z","iopub.status.idle":"2023-06-21T17:13:41.69634Z","shell.execute_reply.started":"2023-06-21T17:13:41.512697Z","shell.execute_reply":"2023-06-21T17:13:41.695345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wordnet","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:41.697716Z","iopub.execute_input":"2023-06-21T17:13:41.698319Z","iopub.status.idle":"2023-06-21T17:13:57.385422Z","shell.execute_reply.started":"2023-06-21T17:13:41.698283Z","shell.execute_reply":"2023-06-21T17:13:57.384101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import wordnet\nnltk.corpus.wordnet\n#from wordnet import Dictionary\nnltk.download('wordnet')\n## To unzip the corpora wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:57.388735Z","iopub.execute_input":"2023-06-21T17:13:57.389582Z","iopub.status.idle":"2023-06-21T17:13:58.74743Z","shell.execute_reply.started":"2023-06-21T17:13:57.389537Z","shell.execute_reply":"2023-06-21T17:13:58.746239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\ncorpus=[]\nwnl=WordNetLemmatizer()\n\nfor i in range(len(df)):\n    review=re.sub('[^a-zA-Z]',\" \",df[\"message\"][i])\n    review=review.lower().split()\n    review=[wnl.lemmatize(word) for word in review if not word in stopwords.words('english')]\n    review=\" \".join(review)\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:13:58.749583Z","iopub.execute_input":"2023-06-21T17:13:58.749997Z","iopub.status.idle":"2023-06-21T17:14:13.169705Z","shell.execute_reply.started":"2023-06-21T17:13:58.749958Z","shell.execute_reply":"2023-06-21T17:14:13.168722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import *\nmodel=CountVectorizer(max_features=6000)\nX=model.fit_transform(corpus).toarray()\nX,X.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:14:13.172477Z","iopub.execute_input":"2023-06-21T17:14:13.177296Z","iopub.status.idle":"2023-06-21T17:14:13.407606Z","shell.execute_reply.started":"2023-06-21T17:14:13.177257Z","shell.execute_reply":"2023-06-21T17:14:13.406453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:14:13.409162Z","iopub.execute_input":"2023-06-21T17:14:13.412845Z","iopub.status.idle":"2023-06-21T17:14:13.424273Z","shell.execute_reply.started":"2023-06-21T17:14:13.412814Z","shell.execute_reply":"2023-06-21T17:14:13.42316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import *\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\nbnb = BernoulliNB()\ny_pred = bnb.fit(X_train, y_train).predict(X_test)\naccuracy_score(y_test, y_pred),confusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T17:14:13.425778Z","iopub.execute_input":"2023-06-21T17:14:13.428665Z","iopub.status.idle":"2023-06-21T17:14:13.967806Z","shell.execute_reply.started":"2023-06-21T17:14:13.428625Z","shell.execute_reply":"2023-06-21T17:14:13.966792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accuracy for this model is 97.78%","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\n 1. for this dataset the Text representation Model TF-IDF is giving Better results than Bag of Words Model\n 2. The Bernoulli Naive Bayes  Model is giving Best results \n 3. Optimal Number of Top features in the text representation is 6000\n ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[]}]}